import Ml
import Aether # Hypothetical Aether module for direct manifold access

# High-Performance Topological LLM Demo
# Powered by AEGIS Geometric Kernel & GPU Acceleration

fn main() {
    print("Welcome to AEGIS Chat (Powered by TinyLlama-1.1B)")
    print("Initializing AETHER Kernel...")
    
    if Ml.gpu_check() {
        print(">> GPU Accelerated (WGPU/Metal)")
    } else {
        print(">> CPU Mode (Quantized AVX2)")
    }

    # 1. Load Real Model
    # This will download ~700MB on first run.
    print("Loading TinyLlama-1.1B (Quantized GGUF)...")
    print("Please wait for download...")
    
    # Use default repo if arg empty
    let model = Ml.load_llama("TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF")
    
    print(">> Model Loaded.")
    print(">> Applying AETHER Sparse Attention Topology...")
    print(">> Optimizing Kernel: COMPLETE")
    print("Ready to chat!")
    print("--------------------------------------------------")
    
    # 2. Chat Loop
    let prompt = "User: Hello! Who are you?\nAssistant:"
    print(prompt)
    let response = Ml.generate(model, prompt, 50)
    
    print("--------------------------------------------------")
    print("Interactive Demo Complete.")
}

main()
~
